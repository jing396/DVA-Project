{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Clear environment\n",
    "rm(list = ls())\n",
    "\n",
    "if(!require(dplyr)) install.packages(\"dplyr\")\n",
    "if(!require(randomForest)) install.packages(\"randomForest\")\n",
    "if(!require(gbm)) install.packages(\"gbm\")\n",
    "if(!require(gam)) install.packages(\"gam\")\n",
    "if(!require(caret)) install.packages(\"caret\")\n",
    "if(!require(e1071)) install.packages(\"e1071\")\n",
    "if(!require(ROCR)) install.packages(\"ROCR\")\n",
    "if(!require(themis)) install.packages(\"themis\") \n",
    "if(!require(patchwork)) install.packages(\"patchwork\")\n",
    "# if (!requireNamespace(\"ROCR\", quietly = TRUE)) {install.packages(\"ROCR\")}\n",
    "\n",
    "update.packages(c(\"themis\", \"recipes\"))\n",
    "\n",
    "library('dplyr');\n",
    "\n",
    "#Read the data\n",
    "## please change the file location\n",
    "# setwd('~/Cathy/OMS/Data Mining&Stat Learn - ISYE-7406-OAN/Project/Data');\n",
    "setwd('~/Cathy/OMS/Data & Visual Analytics - CSE6242OAN O01 O3 AO/Project/data/cleaned_data');\n",
    "\n",
    "Raw_Data <- read.csv(file = \"cleaned_data_updated.csv\", header=TRUE, dec=\",\");\n",
    "\n",
    "# Check unique value of contributing factors\n",
    "lapply(Raw_Data[c('CONTRIBUTING_FACTOR_VEHICLE_1', 'CONTRIBUTING_FACTOR_VEHICLE_2')], unique); \n",
    "\n",
    "# Function to group contributing factors \n",
    "group_factors <- function(factors) {\n",
    "  factor_group <- sapply(factors, function(factor) {\n",
    "    if (factor %in% c(\"Driver Inexperience\", \"Driver Inattention/Distraction\", \"Aggressive Driving/Road Rage\", \n",
    "                      \"Texting\", \"Cell Phone (hands-free)\", \"Eating or Drinking\", \"Passenger Distraction\", \"Fell Asleep\", \n",
    "                      \"Backing Unsafely\", \n",
    "                      \"Cell Phone (hand-Held)\", \"Using On Board Navigation Device\", \n",
    "                      \"Other Electronic Device\", \"Listening/Using Headphones\", \n",
    "                      \"Cell Phone (hand-held)\")) {\n",
    "      return(\"Driver Behavior/Inattention\")\n",
    "    } else if (factor %in% c(\"Brakes Defective\", \"Tire Failure/Inadequate\", \"Steering Failure\", \"Headlights Defective\", \n",
    "                             \"Windshield Inadequate\", \"Other Lighting Defects\", \"Tow Hitch Defective\", \"Oversized Vehicle\", \"Accelerator Defective\", \"Driverless/Runaway Vehicle\", \n",
    "                             \"Vehicle Vandalism\", \"Tinted Windows\")) {\n",
    "      return(\"Vehicle Condition/Mechanical Failure\")\n",
    "    } else if (factor %in% c(\"Traffic Control Disregarded\", \"Lane Marking Improper/Inadequate\", \"Traffic Control Device Improper/Non-Working\")) {\n",
    "      return(\"Traffic Control Issues\")\n",
    "    } else if (factor %in% c(\"Pavement Slippery\", \"Pavement Defective\", \"Shoulders Defective/Improper\", \"Obstruction/Debris\")) {\n",
    "      return(\"Road Conditions\")\n",
    "    } else if (factor %in% c(\"Glare\", \"View Obstructed/Limited\")) {\n",
    "      return(\"Environmental Factors\")\n",
    "    } else if (factor %in% c(\"Unsafe Speed\", \"Following Too Closely\", \"Passing Too Closely\", \"Passing or Lane Usage Improper\", \n",
    "                             \"Unsafe Lane Changing\", \"Failure to Yield Right-of-Way\", \"Failure to Keep Right\", \"Turning Improperly\", \"Oversized Vehicle\", \n",
    "                             \"Alcohol Involvement\", \"Drugs (illegal)\", \"Drugs (Illegal)\", \"Prescription Medication\")) {\n",
    "      return(\"Traffic Violations/Unsafe Driving\")\n",
    "    } else if (factor %in% c(\"Pedestrian/Bicyclist/Other Pedestrian Error/Confusion\", \"Reaction to Uninvolved Vehicle\", \n",
    "                             \"Reaction to Other Uninvolved Vehicle\", \"Animals Action\", \"Other Vehicular\", \"Outside Car Distraction\")) {\n",
    "      return(\"Pedestrian/Other Vehicle Involvement\")\n",
    "    } else if (factor %in% c(\"Unspecified\", \"MISSING\", \"80\", \"1\")) {\n",
    "      return(\"Unspecified or Missing\")\n",
    "    } else if (factor %in% c(\"Illness\", \"Illnes\", \"Lost Consciousness\", \"Physical Disability\", \"Fatigued/Drowsy\")) {\n",
    "      return(\"Health Issues\")\n",
    "    } else {\n",
    "      return(\"Other\")\n",
    "    }\n",
    "  })\n",
    "  \n",
    "  return(factor_group)\n",
    "}\n",
    "\n",
    "# # Filter rows where 'GROUPED_FACTOR_VEHICLE_1' is categorized as 'Other'\n",
    "# other_vehicle_1 <- Raw_Data[Raw_Data$GROUPED_FACTOR_VEHICLE_1 == \"Other\", ]\n",
    "# unique_values_vehicle_1 <- unique(other_vehicle_1$CONTRIBUTING_FACTOR_VEHICLE_1)\n",
    "# \n",
    "# other_vehicle_2 <- Raw_Data[Raw_Data$GROUPED_FACTOR_VEHICLE_2 == \"Other\", ]\n",
    "# unique_values_vehicle_2 <- unique(other_vehicle_2$CONTRIBUTING_FACTOR_VEHICLE_2)\n",
    "# \n",
    "# unique_others <- unique(c(unique_values_vehicle_1, unique_values_vehicle_2))\n",
    "# print(unique_others)\n",
    "\n",
    "Raw_Data$GROUPED_FACTOR_VEHICLE_1 <- group_factors(Raw_Data$CONTRIBUTING_FACTOR_VEHICLE_1)\n",
    "Raw_Data$GROUPED_FACTOR_VEHICLE_2 <- group_factors(Raw_Data$CONTRIBUTING_FACTOR_VEHICLE_2)\n",
    "\n",
    "# lapply(Raw_Data[c('GROUPED_FACTOR_VEHICLE_1', 'GROUPED_FACTOR_VEHICLE_2')], unique); \n",
    "\n",
    "# Raw_Data %>%\n",
    "#   group_by(GROUPED_FACTOR_VEHICLE_1) %>%\n",
    "#   summarise(count = n());\n",
    "# \n",
    "# Raw_Data %>%\n",
    "#   group_by(GROUPED_FACTOR_VEHICLE_2) %>%\n",
    "#   summarise(count = n());\n",
    "\n",
    "# Convert data types\n",
    "\n",
    "# Check if the \"Data\" data frame exists\n",
    "if (exists(\"Data\")) {\n",
    "  # If it exists, remove it\n",
    "  rm(Data)\n",
    "  print(\"Data data frame dropped.\")\n",
    "} else {\n",
    "  print(\"Data data frame does not exist.\")\n",
    "}\n",
    "\n",
    "\n",
    "Data <- Raw_Data %>%\n",
    "  mutate(\n",
    "    NUMBER_OF_PERSONS_INJURED = as.numeric(NUMBER_OF_PERSONS_INJURED),\n",
    "    NUMBER_OF_PERSONS_KILLED = as.numeric(NUMBER_OF_PERSONS_KILLED),\n",
    "    LATITUDE_CRASH = as.numeric(LATITUDE_CRASH),\n",
    "    LONGITUDE_CRASH = as.numeric(LONGITUDE_CRASH),\n",
    "    PRCP = as.numeric(PRCP),\n",
    "    SNOW = as.numeric(SNOW),\n",
    "    SNWD = as.numeric(SNWD),\n",
    "    TMAX = as.numeric(TMAX),\n",
    "    TMIN = as.numeric(TMIN),\n",
    "    BOROUGH = as.numeric(factor(BOROUGH)),  # Convert categorical to factor\n",
    "    ZIP_CODE = as.numeric(factor(ZIP_CODE)),\n",
    "    GROUPED_FACTOR_VEHICLE_1 = as.numeric(factor(GROUPED_FACTOR_VEHICLE_1)),\n",
    "    GROUPED_FACTOR_VEHICLE_2 = as.numeric(factor(GROUPED_FACTOR_VEHICLE_2)),\n",
    "    VEHICLE_1_TYPE_REGROUP = as.numeric(factor(VEHICLE_TYPE_CODE_1_REGROUP)),\n",
    "    VEHICLE_2_TYPE_REGROUP = as.numeric(factor(VEHICLE_TYPE_CODE_2_REGROUP)),\n",
    "    RUSH_HOUR = as.numeric(factor(RUSH_HOUR)),\n",
    "    SEASON = as.numeric((factor(SEASON)))\n",
    "  ) %>%\n",
    "  select(-CONTRIBUTING_FACTOR_VEHICLE_1, -CONTRIBUTING_FACTOR_VEHICLE_2, -VEHICLE_TYPE_CODE_1_REGROUP, -VEHICLE_TYPE_CODE_2_REGROUP) \n",
    "\n",
    "\n",
    "# Create a dummy variable\n",
    "Data$DUMMY_INJURED <- ifelse(Data$NUMBER_OF_PERSONS_INJURED > 0, 1, 0);\n",
    "Data$DUMMY_KILLED <- ifelse(Data$NUMBER_OF_PERSONS_KILLED > 0, 1, 0);\n",
    "Data <- Data %>% select(DUMMY_KILLED, DUMMY_INJURED, everything());\n",
    "# head(Data);\n",
    "\n",
    "# check unique value of dummy variables and their distribution\n",
    "lapply(Data[c('DUMMY_KILLED', 'DUMMY_INJURED')], unique); \n",
    "\n",
    "\n",
    "Data %>%\n",
    "  group_by(DUMMY_KILLED) %>%\n",
    "  summarise(count = n());\n",
    "# DUMMY_KILLED   count\n",
    "# <dbl>   <int>\n",
    "#  0 1648023\n",
    "#  1    2425\n",
    "\n",
    "\n",
    "Data %>%\n",
    "  group_by(DUMMY_INJURED) %>%\n",
    "  summarise(count = n());\n",
    "# DUMMY_INJURED   count\n",
    "# <dbl>   <int>\n",
    "#  0 1247274\n",
    "#  1  403174\n",
    "\n",
    "\n",
    "# View the updated dataframe with new columns\n",
    "head(Raw_Data)\n",
    "\n",
    "########################EDA######################################################\n",
    "#-------------------------------------------------------------------------------#\n",
    "# EDA_Outliers \n",
    "# boxplot(Data$YEAR);\n",
    "# boxplot.stats(Data$YEAR)$out;\n",
    "\n",
    "\n",
    "# EDA correlation matrix\n",
    "# colnames(Data)\n",
    "Data[rowSums(is.na(Data)) > 0, ]\n",
    "\n",
    "Cor_Data <- Data %>%\n",
    "  select(-c('ON_STREET_NAME',\n",
    "            'CROSS_STREET_NAME',\n",
    "            'CRASH_DATETIME',\n",
    "            'NUMBER_OF_PEDESTRIANS_INJURED', \n",
    "            'NUMBER_OF_PEDESTRIANS_KILLED',\n",
    "            'NUMBER_OF_PERSONS_INJURED', \n",
    "            'NUMBER_OF_PERSONS_KILLED',\n",
    "            'NUMBER_OF_CYCLIST_INJURED',\n",
    "            'NUMBER_OF_CYCLIST_KILLED',\n",
    "            'NUMBER_OF_MOTORIST_INJURED',\n",
    "            'NUMBER_OF_MOTORIST_KILLED',\n",
    "            'VEHICLE_TYPE_CODE_1',\n",
    "            'VEHICLE_TYPE_CODE_2',\n",
    "            'COLLISION_ID', \n",
    "            'STATION',\n",
    "            'LATITUDE_WEATHER_STATION',\n",
    "            'LONGITUDE_WEATHER_STATION',\n",
    "            'DISTANCE_FROM_WEATHER_STATION',\n",
    "            'NAME',                     \n",
    "            'ELEVATION'))\n",
    "\n",
    "dim(Cor_Data); \n",
    "# 1650448      22\n",
    "head(Cor_Data); \n",
    "\n",
    "Cor_Data %>%\n",
    "  group_by(DUMMY_KILLED) %>%\n",
    "  summarise (Count=n())\n",
    "# # A tibble: 2 Ã— 2\n",
    "# DUMMY_KILLED   Count\n",
    "# <dbl>   <int>\n",
    "# 0 1648023\n",
    "# 1    2425\n",
    "\n",
    "2425/(2425+1648023) # [1] 0.001469298\n",
    "\n",
    "summary(Cor_Data)\n",
    "\n",
    "# Scatter_Matrix\n",
    "\n",
    "corr_data = cor(Cor_Data);\n",
    "corr_data;\n",
    "library(corrplot);\n",
    "plot.new()\n",
    "dev.off()\n",
    "corrplot(corr_data, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(corr_data, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(corr_data, method=\"ellipse\",tl.cex = 0.5, cl.cex = 0.5); \n",
    "\n",
    "# corrplot for dummy_killed vs. predictors\n",
    "cor_data_Road = cor(Data %>% select(c('DUMMY_KILLED','RUSH_HOUR','BOROUGH', 'ZIP_CODE')));\n",
    "corrplot(cor_data_Road, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Road, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_VEHICLE = cor(Data %>% select(c('DUMMY_KILLED','RUSH_HOUR','VEHICLE_TYPE_CODE_1_REGROUP','VEHICLE_TYPE_CODE_1_REGROUP','BOROUGH')));\n",
    "corrplot(cor_data_VEHICLE, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_VEHICLE, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_Weather = cor(Data %>% select(c('DUMMY_KILLED','RUSH_HOUR','SEASON','PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN')));\n",
    "corrplot(cor_data_Weather, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Weather, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_Road_Weather = cor(Data %>% select(c('DUMMY_KILLED','RUSH_HOUR','SEASON','VEHICLE_TYPE_CODE_1_REGROUP','VEHICLE_TYPE_CODE_1_REGROUP','BOROUGH', 'ZIP_CODE','PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN')));\n",
    "corrplot(cor_data_Road_Weather, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Road_Weather, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "# corrplot for dummy_injured vs. predictors\n",
    "cor_data_Road2 = cor(Data %>% select(c('DUMMY_INJURED','RUSH_HOUR','BOROUGH', 'ZIP_CODE')));\n",
    "corrplot(cor_data_Road2, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Road2, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_VEHICLE2 = cor(Data %>% select(c('DUMMY_INJURED','RUSH_HOUR','VEHICLE_TYPE_CODE_1_REGROUP','VEHICLE_TYPE_CODE_1_REGROUP','BOROUGH')));\n",
    "corrplot(cor_data_VEHICLE2, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_VEHICLE2, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_Weather2 = cor(Data %>% select(c('DUMMY_INJURED','RUSH_HOUR','SEASON','PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN')));\n",
    "corrplot(cor_data_Weather2, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Weather2, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "cor_data_Road_Weather2 = cor(Data %>% select(c('DUMMY_INJURED','RUSH_HOUR','SEASON','VEHICLE_TYPE_CODE_1_REGROUP','VEHICLE_TYPE_CODE_1_REGROUP','BOROUGH', 'ZIP_CODE','PRCP', 'SNOW', 'SNWD', 'TMAX', 'TMIN')));\n",
    "corrplot(cor_data_Road_Weather2, method=\"color\", tl.cex = 0.5, cl.cex = 0.5);\n",
    "corrplot(cor_data_Road_Weather2, method=\"number\", tl.cex = 0.5, number.cex = 0.5, cl.cex = 0.5);\n",
    "\n",
    "# boxplots\n",
    "# head(Data)\n",
    "library(ggplot2)\n",
    "library(GGally)\n",
    "\n",
    "font_size <- 10 # Set your desired font size\n",
    "\n",
    "for (i in 2:dim(Cor_Data)[2]) {\n",
    "  print(\n",
    "    ggplot(Cor_Data, aes(x = as.factor(DUMMY_KILLED), y = Cor_Data[, i])) +\n",
    "      geom_boxplot() +\n",
    "      labs(y = colnames(Cor_Data)[i]) +\n",
    "      theme(\n",
    "        axis.title = element_text(size = font_size),\n",
    "        axis.text = element_text(size = font_size),\n",
    "        plot.title = element_text(size = font_size),\n",
    "        text = element_text(size = font_size)\n",
    "      )\n",
    "  )\n",
    "}\n",
    "# According to boxplots, the following predictors have different distribution for dummy_killed 1 vs. 0: \n",
    "# significant difference: BOROUGH, RUSH_HOUR, YEAR, VEHICLE_2_TYPE_REGROUP, HOUR\n",
    "# Moderate difference: LATITUDE_CRASH, GROUPED_FACTOR_VEHICLE_1, SNOW, SNWD, PCRP, TMIN, HOUR\n",
    "# Minimum difference: ZIP_CODE, lONGTITUDE_CRASH, TMAX, MONTH, DAY, VEHICLE_1_TYPE_REGROUP, SEASON, GROUPED_FACTOR_VEHICLE_2, WEEKDAY\n",
    "\n",
    "##-------------------------------------------------------------------------##\n",
    "######################END of EDA#############################################\n",
    "\n",
    "##############Prepare datasets for modeling##################################\n",
    "#---------------------------------------------------------------------------#\n",
    "\n",
    "# Load necessary libraries\n",
    "library(smotefamily)\n",
    "library(ROCR)\n",
    "library(caret)  # For confusionMatrix\n",
    "\n",
    "# Define helper functions for repeated operations\n",
    "get_sample_data <- function(data, prop = 0.10, seed = 123) {\n",
    "  set.seed(seed)\n",
    "  n_rows <- nrow(data)\n",
    "  random_indices <- sample(1:n_rows, floor(prop * n_rows), replace = FALSE)\n",
    "  return(data[random_indices, ])\n",
    "}\n",
    "\n",
    "# Clean and preprocess the data\n",
    "Data1 <- Cor_Data\n",
    "dim(Data1)  # [1] 1650448 22\n",
    "\n",
    "# Subset the data, remove multicollinear or irrelevant columns\n",
    "Data2 <- subset(Cor_Data, select = c(DUMMY_KILLED, DUMMY_INJURED, BOROUGH, RUSH_HOUR, YEAR,\n",
    "                                     VEHICLE_1_TYPE_REGROUP, VEHICLE_2_TYPE_REGROUP,\n",
    "                                     LATITUDE_CRASH, GROUPED_FACTOR_VEHICLE_1, SNOW, SNWD, PRCP, TMIN))\n",
    "dim(Data2)  # [1] 1650448 13\n",
    "\n",
    "# -------------------- Split Data into Training and Testing Sets --------------------\n",
    "set.seed(6242)\n",
    "n <- nrow(Data1)\n",
    "n1 <- round(n * 0.20)  # 20% for testing\n",
    "\n",
    "flag <- sort(sample(1:n, n1))\n",
    "Data1_train <- Data1[-flag, ]\n",
    "Data1_test <- Data1[flag, ]\n",
    "Data2_train <- Data2[-flag, ]\n",
    "Data2_test <- Data2[flag, ]\n",
    "\n",
    "# Sample 10% of data for smaller subset processing\n",
    "Data1_train_10 <- get_sample_data(Data1_train)\n",
    "Data1_test_10 <- get_sample_data(Data1_test)\n",
    "Data2_train_10 <- get_sample_data(Data2_train)\n",
    "Data2_test_10 <- get_sample_data(Data2_test)\n",
    "\n",
    "# Remove the highly correlated column (column 2 DUMMY_INJURED)\n",
    "remove_column <- function(data) {\n",
    "  data[, -2]\n",
    "}\n",
    "\n",
    "Data1_train_processed <- remove_column(Data1_train)\n",
    "Data1_test_processed <- remove_column(Data1_test)\n",
    "Data1_train_10_processed <- remove_column(Data1_train_10)\n",
    "Data1_test_10_processed <- remove_column(Data1_test_10)\n",
    "Data2_train_processed <- remove_column(Data2_train)\n",
    "Data2_test_processed <- remove_column(Data2_test)\n",
    "Data2_train_10_processed <- remove_column(Data2_train_10)\n",
    "Data2_test_10_processed <- remove_column(Data2_test_10)\n",
    "\n",
    "# Extract true response values\n",
    "y1 <- Data1_train$DUMMY_KILLED\n",
    "y2 <- Data1_test$DUMMY_KILLED\n",
    "y3 <- Data2_train$DUMMY_KILLED\n",
    "y4 <- Data2_test$DUMMY_KILLED\n",
    "\n",
    "y1_10 <- Data1_train_10$DUMMY_KILLED\n",
    "y2_10 <- Data1_test_10$DUMMY_KILLED\n",
    "y3_10 <- Data2_train_10$DUMMY_KILLED\n",
    "y4_10 <- Data2_test_10$DUMMY_KILLED\n",
    "\n",
    "# -------------------- Function to Calculate F1 Score --------------------\n",
    "F1_Score <- function(predicted, actual) {\n",
    "  tp <- sum(predicted == 1 & actual == 1)\n",
    "  fp <- sum(predicted == 1 & actual == 0)\n",
    "  fn <- sum(predicted == 0 & actual == 1)\n",
    "  \n",
    "  # Check for zero division errors and handle them\n",
    "  precision <- ifelse((tp + fp) == 0, 0, tp / (tp + fp))\n",
    "  recall <- ifelse((tp + fn) == 0, 0, tp / (tp + fn))\n",
    "  \n",
    "  # Handle edge case where precision and recall are both zero\n",
    "  f1 <- ifelse((precision + recall) == 0, 0, 2 * precision * recall / (precision + recall))\n",
    "  \n",
    "  return(f1)\n",
    "}\n",
    "\n",
    "# -------------------- Logistic Regression with SMOTE --------------------\n",
    "logistic_regression_smote <- function(train_data, test_data, response_col = \"DUMMY_KILLED\") {\n",
    "  # Separate features and target\n",
    "  X_train <- train_data[, -which(names(train_data) == response_col)]\n",
    "  y_train <- train_data[[response_col]]\n",
    "  \n",
    "  # Apply SMOTE\n",
    "  smote_output <- SMOTE(X_train, y_train, K = 5)\n",
    "  train_smote <- smote_output$data\n",
    "  colnames(train_smote)[ncol(train_smote)] <- response_col\n",
    "  train_smote[[response_col]] <- as.numeric(as.character(train_smote[[response_col]]))\n",
    "  \n",
    "  # Logistic Regression Model\n",
    "  mod_glm_test <- glm(DUMMY_KILLED ~ ., family = binomial, data = train_smote, \n",
    "                      weights = ifelse(train_smote$DUMMY_KILLED == 1, 1, 10))\n",
    "  \n",
    "  # Define cutoff range\n",
    "  cutoff_values <- seq(0.1, 0.9, 0.1)\n",
    "  Train_Error_glm <- NULL\n",
    "  Test_Error_glm <- NULL\n",
    "  \n",
    "  # Calculate Train and Test Errors for different cutoffs\n",
    "  for (c in cutoff_values) {\n",
    "    y1hat <- ifelse(predict(mod_glm_test, train_smote[, -which(names(train_smote) == response_col)], type = \"response\") < c, 0, 1)\n",
    "    y2hat <- ifelse(predict(mod_glm_test, test_data[, -which(names(test_data) == response_col)], type = \"response\") < c, 0, 1)\n",
    "    Train_Error_glm <- c(Train_Error_glm, mean(y1hat != train_smote$DUMMY_KILLED))\n",
    "    Test_Error_glm <- c(Test_Error_glm, mean(y2hat != test_data[[response_col]]))\n",
    "  }\n",
    "  \n",
    "  # Plot and evaluate errors\n",
    "  plot(cutoff_values, Test_Error_glm)\n",
    "  return(list(Train_Error_glm = Train_Error_glm, Test_Error_glm = Test_Error_glm))\n",
    "}\n",
    "\n",
    "# Apply Logistic Regression with SMOTE for Data1 and Data2\n",
    "logistic_regression_smote(Data1_train_10_processed, Data1_test_10_processed, response_col = \"DUMMY_KILLED\")\n",
    "# $Train_Error_glm\n",
    "# [1] 0.3313495 0.4450501 0.4960949 0.5003509 0.4998540 0.4998009 0.4998009 0.4998009 0.4998009\n",
    "# $Test_Error_glm\n",
    "# [1] 0.346238905 0.069617377 0.011299948 0.002665940 0.001363265 0.001302675 0.001302675\n",
    "# [8] 0.001302675 0.001302675\n",
    "\n",
    "logistic_regression_smote(Data2_train_10_processed, Data2_test_10_processed, response_col = \"DUMMY_KILLED\")\n",
    "# $Train_Error_glm\n",
    "# [1] 0.3271884 0.4510699 0.4956056 0.4998502 0.4998009 0.4998009 0.4998009 0.4998009 0.4998009\n",
    "# $Test_Error_glm\n",
    "# [1] 0.362658669 0.067617922 0.006695144 0.001423854 0.001332970 0.001302675 0.001302675\n",
    "# [8] 0.001302675 0.001302675\n",
    "\n",
    "# -------------------- Logistic Regression with Stepwise Selection --------------------\n",
    "# Logistic Regression with Stepwise Selection and SMOTE for imbalanced data\n",
    "logistic_regression_step_smote <- function(train_data, test_data, response_col = \"DUMMY_KILLED\") {\n",
    "  \n",
    "  # Separate features and target\n",
    "  X_train <- train_data[, -which(names(train_data) == response_col)]\n",
    "  y_train <- train_data[[response_col]]\n",
    "  \n",
    "  # Apply SMOTE to balance the data\n",
    "  smote_output <- SMOTE(X_train, y_train, K = 5)\n",
    "  train_smote <- smote_output$data\n",
    "  colnames(train_smote)[ncol(train_smote)] <- response_col\n",
    "  train_smote[[response_col]] <- as.factor(train_smote[[response_col]])\n",
    "  \n",
    "  # Check NA values.\n",
    "  # print(paste(\"NA values in train_smote:\", sum(is.na(train_smote))))\n",
    "  # print(paste(\"NA values in test_data:\", sum(is.na(test_data))))\n",
    "  \n",
    "  # Fit the logistic regression model using glm (with weights for the imbalance)\n",
    "  mod_glms_test <- glm(as.factor(DUMMY_KILLED) ~ ., family = binomial, data = train_smote,\n",
    "                       weights = ifelse(train_smote$DUMMY_KILLED == 1, 1, 10))\n",
    "  \n",
    "  # Perform stepwise feature selection using base R's step() function\n",
    "  mod_glms_test_step <- stats::step(mod_glms_test, direction = \"both\")\n",
    "  \n",
    "  # Define cutoff values for testing\n",
    "  cutoff_values <- seq(0.1, 0.9, 0.1)\n",
    "  Train_Error_glms <- NULL\n",
    "  Test_Error_glms <- NULL\n",
    "  \n",
    "  # Calculate Train and Test Errors for different cutoff values\n",
    "  for (c in cutoff_values) {\n",
    "    # Train predictions\n",
    "    train_preds <- predict(mod_glms_test_step, train_smote[, -which(names(train_smote) == response_col)], type = \"response\")\n",
    "    y1hat <- ifelse(train_preds < c, 0, 1)\n",
    "    \n",
    "    # Test predictions\n",
    "    test_preds <- predict(mod_glms_test_step, test_data[, -which(names(test_data) == response_col)], type = \"response\")\n",
    "    y2hat <- ifelse(test_preds < c, 0, 1)\n",
    "    \n",
    "    # Store the error for the current cutoff value\n",
    "    Train_Error_glms <- c(Train_Error_glms, mean(y1hat != train_smote[[response_col]]))\n",
    "    Test_Error_glms <- c(Test_Error_glms, mean(y2hat != test_data[[response_col]]))\n",
    "  }\n",
    "  \n",
    "  # Plot and evaluate errors\n",
    "  plot(cutoff_values, Test_Error_glms, type = \"b\", col = \"blue\", pch = 19,\n",
    "       xlab = \"Cutoff Value\", ylab = \"Test Error\", main = \"Test Error vs Cutoff Values\")\n",
    "  \n",
    "  # Return error values\n",
    "  return(list(Train_Error_glms = Train_Error_glms, Test_Error_glms = Test_Error_glms))\n",
    "}\n",
    "\n",
    "# Apply Logistic Regression with Stepwise Selection and SMOTE for Data1 and Data2\n",
    "logistic_regression_step_smote(Data1_train_10_processed, Data1_test_10_processed, response_col = \"DUMMY_KILLED\")\n",
    "# Start:  AIC=802476\n",
    "# as.factor(DUMMY_KILLED) ~ BOROUGH + ZIP_CODE + LATITUDE_CRASH + \n",
    "#   LONGITUDE_CRASH + PRCP + SNOW + SNWD + TMAX + TMIN + YEAR + \n",
    "#   MONTH + DAY + HOUR + RUSH_HOUR + SEASON + WEEKDAY + GROUPED_FACTOR_VEHICLE_1 + \n",
    "#   GROUPED_FACTOR_VEHICLE_2 + VEHICLE_1_TYPE_REGROUP + VEHICLE_2_TYPE_REGROUP\n",
    "# \n",
    "# Df Deviance    AIC\n",
    "# - DAY                       1   802434 802474\n",
    "# - TMIN                      1   802436 802476\n",
    "# <none>                          802434 802476\n",
    "# - SNWD                      1   802444 802484\n",
    "# - LATITUDE_CRASH            1   802486 802526\n",
    "# - TMAX                      1   802509 802549\n",
    "# - RUSH_HOUR                 1   802579 802619\n",
    "# - ZIP_CODE                  1   802791 802831\n",
    "# - VEHICLE_1_TYPE_REGROUP    1   802895 802935\n",
    "# - PRCP                      1   803086 803126\n",
    "# - SNOW                      1   803151 803191\n",
    "# - WEEKDAY                   1   803203 803243\n",
    "# - MONTH                     1   803291 803331\n",
    "# - SEASON                    1   803611 803651\n",
    "# - LONGITUDE_CRASH           1   804173 804213\n",
    "# - BOROUGH                   1   804672 804712\n",
    "# - GROUPED_FACTOR_VEHICLE_2  1   805618 805658\n",
    "# - GROUPED_FACTOR_VEHICLE_1  1   806363 806403\n",
    "# - HOUR                      1   807117 807157\n",
    "# - YEAR                      1   815625 815665\n",
    "# - VEHICLE_2_TYPE_REGROUP    1   839858 839898\n",
    "# \n",
    "# Step:  AIC=802474.4\n",
    "# as.factor(DUMMY_KILLED) ~ BOROUGH + ZIP_CODE + LATITUDE_CRASH + \n",
    "#   LONGITUDE_CRASH + PRCP + SNOW + SNWD + TMAX + TMIN + YEAR + \n",
    "#   MONTH + HOUR + RUSH_HOUR + SEASON + WEEKDAY + GROUPED_FACTOR_VEHICLE_1 + \n",
    "#   GROUPED_FACTOR_VEHICLE_2 + VEHICLE_1_TYPE_REGROUP + VEHICLE_2_TYPE_REGROUP\n",
    "# \n",
    "# Df Deviance    AIC\n",
    "# - TMIN                      1   802436 802474\n",
    "# <none>                          802434 802474\n",
    "# + DAY                       1   802434 802476\n",
    "# - SNWD                      1   802445 802483\n",
    "# - LATITUDE_CRASH            1   802487 802525\n",
    "# - TMAX                      1   802510 802548\n",
    "# - RUSH_HOUR                 1   802580 802618\n",
    "# - ZIP_CODE                  1   802791 802829\n",
    "# - VEHICLE_1_TYPE_REGROUP    1   802896 802934\n",
    "# - PRCP                      1   803086 803124\n",
    "# - SNOW                      1   803152 803190\n",
    "# - WEEKDAY                   1   803204 803242\n",
    "# - MONTH                     1   803291 803329\n",
    "# - SEASON                    1   803613 803651\n",
    "# - LONGITUDE_CRASH           1   804174 804212\n",
    "# - BOROUGH                   1   804672 804710\n",
    "# - GROUPED_FACTOR_VEHICLE_2  1   805618 805656\n",
    "# - GROUPED_FACTOR_VEHICLE_1  1   806363 806401\n",
    "# - HOUR                      1   807117 807155\n",
    "# - YEAR                      1   815626 815664\n",
    "# - VEHICLE_2_TYPE_REGROUP    1   839860 839898\n",
    "# \n",
    "# Step:  AIC=802474.2\n",
    "# as.factor(DUMMY_KILLED) ~ BOROUGH + ZIP_CODE + LATITUDE_CRASH + \n",
    "#   LONGITUDE_CRASH + PRCP + SNOW + SNWD + TMAX + YEAR + MONTH + \n",
    "#   HOUR + RUSH_HOUR + SEASON + WEEKDAY + GROUPED_FACTOR_VEHICLE_1 + \n",
    "#   GROUPED_FACTOR_VEHICLE_2 + VEHICLE_1_TYPE_REGROUP + VEHICLE_2_TYPE_REGROUP\n",
    "# \n",
    "# Df Deviance    AIC\n",
    "# <none>                          802436 802474\n",
    "# + TMIN                      1   802434 802474\n",
    "# + DAY                       1   802436 802476\n",
    "# - SNWD                      1   802447 802483\n",
    "# - LATITUDE_CRASH            1   802489 802525\n",
    "# - RUSH_HOUR                 1   802581 802617\n",
    "# - ZIP_CODE                  1   802793 802829\n",
    "# - VEHICLE_1_TYPE_REGROUP    1   802898 802934\n",
    "# - PRCP                      1   803093 803129\n",
    "# - SNOW                      1   803158 803194\n",
    "# - WEEKDAY                   1   803206 803242\n",
    "# - MONTH                     1   803398 803434\n",
    "# - SEASON                    1   803769 803805\n",
    "# - LONGITUDE_CRASH           1   804176 804212\n",
    "# - TMAX                      1   804379 804415\n",
    "# - BOROUGH                   1   804673 804709\n",
    "# - GROUPED_FACTOR_VEHICLE_2  1   805621 805657\n",
    "# - GROUPED_FACTOR_VEHICLE_1  1   806366 806402\n",
    "# - HOUR                      1   807125 807161\n",
    "# - YEAR                      1   815665 815701\n",
    "# - VEHICLE_2_TYPE_REGROUP    1   839861 839897\n",
    "# $Train_Error_glms\n",
    "# [1] 0.3310309 0.4447011 0.4959622 0.5003736 0.4998578 0.4998009 0.4998009 0.4998009 0.4998009\n",
    "# \n",
    "# $Test_Error_glms\n",
    "# [1] 0.345360356 0.069950620 0.011633191 0.002726529 0.001363265 0.001302675 0.001302675\n",
    "# [8] 0.001302675 0.001302675\n",
    "\n",
    "logistic_regression_step_smote(Data2_train_10_processed, Data2_test_10_processed, response_col = \"DUMMY_KILLED\")\n",
    "# Start:  AIC=803090\n",
    "# as.factor(DUMMY_KILLED) ~ BOROUGH + RUSH_HOUR + YEAR + VEHICLE_1_TYPE_REGROUP + \n",
    "#   VEHICLE_2_TYPE_REGROUP + LATITUDE_CRASH + GROUPED_FACTOR_VEHICLE_1 + \n",
    "#   SNOW + SNWD + PRCP + TMIN\n",
    "# \n",
    "# Df Deviance    AIC\n",
    "# <none>                          803066 803090\n",
    "# - SNWD                      1   803225 803247\n",
    "# - LATITUDE_CRASH            1   803367 803389\n",
    "# - PRCP                      1   803444 803466\n",
    "# - VEHICLE_1_TYPE_REGROUP    1   803687 803709\n",
    "# - SNOW                      1   803983 804005\n",
    "# - RUSH_HOUR                 1   804147 804169\n",
    "# - TMIN                      1   804484 804506\n",
    "# - BOROUGH                   1   806271 806293\n",
    "# - GROUPED_FACTOR_VEHICLE_1  1   810793 810815\n",
    "# - YEAR                      1   817792 817814\n",
    "# - VEHICLE_2_TYPE_REGROUP    1   848748 848770\n",
    "# $Train_Error_glms\n",
    "# [1] 0.3268508 0.4511306 0.4954918 0.4998502 0.4998009 0.4998009 0.4998009 0.4998009 0.4998009\n",
    "# \n",
    "# $Test_Error_glms\n",
    "# [1] 0.362567785 0.067496743 0.006786028 0.001423854 0.001332970 0.001302675 0.001302675\n",
    "# [8] 0.001302675 0.001302675\n",
    "\n",
    "## -----------------------------Monte Carlo Cross-Validation--------------------------------\n",
    "# ------------------------------------------------------------------------------------------\n",
    "\n",
    "# Set parameters\n",
    "B <- 100  # Number of loops\n",
    "set.seed(6242)  # Set the seed for randomization\n",
    "\n",
    "# Initialize error storage\n",
    "TRERR1 <- NULL  # Average training errors for dataset 1\n",
    "TEALL1 <- NULL  # Final test errors for dataset 1\n",
    "TRERR2 <- NULL  # Average training errors for dataset 2\n",
    "TEALL2 <- NULL  # Final test errors for dataset 2\n",
    "\n",
    "for (b in 1:B) {\n",
    "  \n",
    "  # Randomly select a set of indices for the test and train split\n",
    "  flag <- sort(sample(1:n, n1))\n",
    "  \n",
    "  # Split data into train and test sets\n",
    "  Data1_train <- Data1[-flag, ]\n",
    "  Data1_test <- Data1[flag, ]\n",
    "  Data2_train <- Data2[-flag, ]\n",
    "  Data2_test <- Data2[flag, ]\n",
    "  \n",
    "  # Define response variables for both datasets\n",
    "  y1 <- Data1$Response\n",
    "  y2 <- Data2$Response\n",
    "  \n",
    "  # Initialize error vectors\n",
    "  TrainErr1_CV <- NULL\n",
    "  TestErr1_CV <- NULL\n",
    "  TrainErr2_CV <- NULL\n",
    "  TestErr2_CV <- NULL\n",
    "  \n",
    "  # Logistic Regression for Dataset 1\n",
    "  mod_glm1_CV <- glm(Response ~ ., family = binomial, data = Data1_train)\n",
    "  c <- 0.5  # Cutoff value\n",
    "  TrainErr1_CV <- c(TrainErr1_CV, mean(ifelse(predict(mod_glm1_CV, Data1_train[, -c(1, 2)], type = 'response') > c, 1, 0) != y1))\n",
    "  TestErr1_CV <- c(TestErr1_CV, mean(ifelse(predict(mod_glm1_CV, Data1_test[, -c(1, 2)], type = 'response') > c, 1, 0) != y2))\n",
    "  \n",
    "  # Logistic Regression for Dataset 2\n",
    "  mod_glm2_CV <- glm(Response ~ ., family = binomial, data = Data2_train)\n",
    "  TrainErr2_CV <- c(TrainErr2_CV, mean(ifelse(predict(mod_glm2_CV, Data2_train[, -c(1, 2)], type = 'response') > c, 1, 0) != y1))\n",
    "  TestErr2_CV <- c(TestErr2_CV, mean(ifelse(predict(mod_glm2_CV, Data2_test[, -c(1, 2)], type = 'response') > c, 1, 0) != y2))\n",
    "  \n",
    "  # Logistic Regression with step() for Dataset 1\n",
    "  mod_glms1_CV <- step(glm(Response ~ ., family = binomial, data = Data1_train))\n",
    "  TrainErr1_CV <- c(TrainErr1_CV, mean(ifelse(predict(mod_glms1_CV, Data1_train[, -c(1, 2)], type = 'response') > c, 1, 0) != y1))\n",
    "  TestErr1_CV <- c(TestErr1_CV, mean(ifelse(predict(mod_glms1_CV, Data1_test[, -c(1, 2)], type = 'response') > c, 1, 0) != y2))\n",
    "  \n",
    "  # Logistic Regression with step() for Dataset 2\n",
    "  mod_glms2_CV <- step(glm(Response ~ ., family = binomial, data = Data2_train))\n",
    "  TrainErr2_CV <- c(TrainErr2_CV, mean(ifelse(predict(mod_glms2_CV, Data2_train[, -c(1, 2)], type = 'response') > c, 1, 0) != y1))\n",
    "  TestErr2_CV <- c(TestErr2_CV, mean(ifelse(predict(mod_glms2_CV, Data2_test[, -c(1, 2)], type = 'response') > c, 1, 0) != y2))\n",
    "  \n",
    "  # Store results for each iteration\n",
    "  TRERR1 <- rbind(TRERR1, TrainErr1_CV)\n",
    "  TEALL1 <- rbind(TEALL1, TestErr1_CV)\n",
    "  TRERR2 <- rbind(TRERR2, TrainErr2_CV)\n",
    "  TEALL2 <- rbind(TEALL2, TestErr2_CV)\n",
    "}\n",
    "\n",
    "# Results summary\n",
    "dim(TEALL1)  # 100 5\n",
    "dim(TEALL2)  # 100 5\n",
    "\n",
    "colnames(TEALL1) <- c(\"Logistic Regression\", \"Logistic Regression 2\")\n",
    "colnames(TEALL2) <- c(\"Logistic Regression\", \"Logistic Regression 2\")\n",
    "\n",
    "# CV result statistics\n",
    "mean_TRERR1 <- apply(TRERR1, 2, mean)\n",
    "mean_TEALL1 <- apply(TEALL1, 2, mean)\n",
    "var_TEALL1 <- apply(TEALL1, 2, var)\n",
    "\n",
    "mean_TRERR2 <- apply(TRERR2, 2, mean)\n",
    "mean_TEALL2 <- apply(TEALL2, 2, mean)\n",
    "var_TEALL2 <- apply(TEALL2, 2, var)\n",
    "\n",
    "# Plot CV Error\n",
    "k <- c(1, 2, 3, 4, 5)\n",
    "\n",
    "# Plot for Dataset 1 and Dataset 2\n",
    "plot(k, mean_TEALL1, xlab = 'Models', ylab = 'CV Error', main = \"CV Error Plot: Dataset 1\")\n",
    "plot(k, mean_TEALL2, xlab = 'Models', ylab = 'CV Error', main = \"CV Error Plot: Dataset 2\")\n",
    "\n",
    "# Combined plot\n",
    "TEALL <- cbind(mean_TEALL1, mean_TEALL2)\n",
    "matplot(k, TEALL, type = \"l\", lty = 1, col = c(\"red\", \"blue\"), xlab = \"k\", ylab = \"CV Error\", main = \"CV Error Plot\")\n",
    "legend(\"topright\", legend = c(\"Dataset 1\", \"Dataset 2\"), col = c(\"red\", \"blue\"), lty = 1, cex = 0.5)\n",
    "\n",
    "## -----------------------------------------------------------------------------\n",
    "# ## Statistical difference testing on Dataset 1\n",
    "# T1_1 <- t.test(TEALL1[,1], TEALL1[,2], paired = TRUE)\n",
    "# T2_1 <- t.test(TEALL1[,1], TEALL1[,3], paired = TRUE)\n",
    "# T3_1 <- t.test(TEALL1[,1], TEALL1[,4], paired = TRUE)\n",
    "# T4_1 <- t.test(TEALL1[,1], TEALL1[,5], paired = TRUE)\n",
    "# \n",
    "# W1_1 <- wilcox.test(TEALL1[,1], TEALL1[,2], paired = TRUE)\n",
    "# W2_1 <- wilcox.test(TEALL1[,1], TEALL1[,3], paired = TRUE)\n",
    "# W3_1 <- wilcox.test(TEALL1[,1], TEALL1[,4], paired = TRUE)\n",
    "# W4_1 <- wilcox.test(TEALL1[,1], TEALL1[,5], paired = TRUE)\n",
    "# \n",
    "# p_values_T1_1 <- T1_1$p.value\n",
    "# p_values_T2_1 <- T2_1$p.value\n",
    "# p_values_T3_1 <- T3_1$p.value\n",
    "# p_values_T4_1 <- T4_1$p.value\n",
    "# \n",
    "# p_values_W1_1 <- W1_1$p.value\n",
    "# p_values_W2_1 <- W2_1$p.value\n",
    "# p_values_W3_1 <- W3_1$p.value\n",
    "# p_values_W4_1 <- W4_1$p.value\n",
    "# \n",
    "# # Results\n",
    "# c(p_values_T1_1, p_values_T2_1, p_values_T3_1, p_values_T4_1)\n",
    "# c(p_values_W1_1, p_values_W2_1, p_values_W3_1, p_values_W4_1)\n",
    "# \n",
    "# # Boxplot for Dataset 1\n",
    "# boxplot(TEALL1)\n",
    "# \n",
    "# ## Statistical difference testing on Dataset 2\n",
    "# T1_2 <- t.test(TEALL2[,1], TEALL2[,2], paired = TRUE)\n",
    "# T2_2 <- t.test(TEALL2[,1], TEALL2[,3], paired = TRUE)\n",
    "# T3_2 <- t.test(TEALL2[,1], TEALL2[,4], paired = TRUE)\n",
    "# T4_2 <- t.test(TEALL2[,1], TEALL2[,5], paired = TRUE)\n",
    "# \n",
    "# W1_2 <- wilcox.test(TEALL2[,1], TEALL2[,2], paired = TRUE)\n",
    "# W2_2 <- wilcox.test(TEALL2[,1], TEALL2[,3], paired = TRUE)\n",
    "# W3_2 <- wilcox.test(TEALL2[,1], TEALL2[,4], paired = TRUE)\n",
    "# W4_2 <- wilcox.test(TEALL2[,1], TEALL2[,5], paired = TRUE)\n",
    "# \n",
    "# p_values_T1_2 <- T1_2$p.value\n",
    "# p_values_T2_2 <- T2_2$p.value\n",
    "# p_values_T3_2 <- T3_2$p.value\n",
    "# p_values_T4_2 <- T4_2$p.value\n",
    "# \n",
    "# p_values_W1_2 <- W1_2$p.value\n",
    "# p_values_W2_2 <- W2_2$p.value\n",
    "# p_values_W3_2 <- W3_2$p.value\n",
    "# p_values_W4_2 <- W4_2$p.value\n",
    "# \n",
    "# # Results\n",
    "# c(p_values_T1_2, p_values_T2_2, p_values_T3_2, p_values_T4_2)\n",
    "# c(p_values_W1_2, p_values_W2_2, p_values_W3_2, p_values_W4_2)\n",
    "# \n",
    "# # Boxplot for Dataset 2\n",
    "# boxplot(TEALL2)\n",
    "\n",
    "#####################Random Forest###########################################\n",
    "# --------------------------- Random Forest ---------------------------------\n",
    "# 1. **Random Forest with SMOTE and Variable Importance**\n",
    "# ----------------------------------Random Forest with Parameter Tuning-------------------------------------\n",
    "library(randomForest)\n",
    "library(caret)\n",
    "library(e1071)\n",
    "update.packages(\"smotefamily\")\n",
    "library(smotefamily) # For SMOTE\n",
    "library(ROCR)\n",
    "\n",
    "# 1. Prepare Data and Apply SMOTE from smotefamily\n",
    "X_train_10 <- Data1_train_10_processed[, -which(names(Data1_train_10_processed) == \"DUMMY_KILLED\")]\n",
    "y_train_10 <- Data1_train_10_processed$DUMMY_KILLED\n",
    "\n",
    "# Apply SMOTE using smotefamily\n",
    "smote_output <- SMOTE(X_train_10, y_train_10, K = 5) # K is the number of nearest neighbors\n",
    "train_smote <- smote_output$data\n",
    "train_smote$class <- as.factor(train_smote$class) # Ensure it's a factor\n",
    "head(train_smote)\n",
    "\n",
    "# 2. Set up cross-validation and parameter tuning grid for Random Forest\n",
    "set.seed(111)\n",
    "\n",
    "# Define tuning grid for Random Forest\n",
    "tuneGrid <- expand.grid(.mtry = c(3, 5)) # Number of variables to be randomly sampled at each split\n",
    "ntrees <- c(100, 200, 300)    \n",
    "nodesize <- c(1, 5, 10)\n",
    "param_grid <- expand.grid(ntrees = ntrees,\n",
    "                          nodesize = nodesize)\n",
    "\n",
    "# 3. Set up cross-validation with SMOTE resampling\n",
    "cv_folds <- createFolds(train_smote$class, k = 5, returnTrain = TRUE)\n",
    "\n",
    "ctrl <- trainControl(method = \"cv\",\n",
    "                     number = 3,\n",
    "                     search = 'grid',\n",
    "                     classProbs = TRUE,\n",
    "                     savePredictions = \"final\",\n",
    "                     index = cv_folds,\n",
    "                     summaryFunction = twoClassSummary)\n",
    "\n",
    "# 4. Train Random Forest model with tuning using caret\n",
    "# rf_tune <- train(\n",
    "#   make.names(class) ~ .,  # Formula\n",
    "#   data = train_smote,  # Training data\n",
    "#   method = \"rf\",  # Random Forest method\n",
    "#   trControl = ctrl,  # Cross-validation control\n",
    "#   tuneGrid = tuneGrid,  # Parameter tuning grid\n",
    "#   importance = TRUE,  # Calculate variable importance\n",
    "#   metric = \"ROC\"  # Optimize for ROC AUC\n",
    "# )\n",
    "\n",
    "data_maxnode <- vector(\"list\", nrow(param_grid))\n",
    "for(i in 1:nrow(param_grid)){\n",
    "  ntree <- param_grid[i,1]\n",
    "  nodesize <- param_grid[i,2]\n",
    "  set.seed(333)\n",
    "  rf_model <- train(make.names(class)~.,\n",
    "                    data = train_smote,\n",
    "                    method = \"rf\",\n",
    "                    importance=TRUE,\n",
    "                    metric = \"ROC\",\n",
    "                    trControl = ctrl,\n",
    "                    tuneGrid = tuneGrid,\n",
    "                    ntree = ntree,\n",
    "                    nodesize = nodesize)\n",
    "  data_maxnode[[i]] <- rf_model\n",
    "}\n",
    "\n",
    "names(data_maxnode) <- paste(\"ntrees:\", param_grid$ntrees,\n",
    "                              \"nodesize:\", param_grid$nodesize)\n",
    "\n",
    "results_mtry <- resamples(data_maxnode)\n",
    "\n",
    "summary(results_mtry)\n",
    "# Call:\n",
    "#   summary.resamples(object = results_mtry)\n",
    "# \n",
    "# Models: ntrees: 100 nodesize: 1, ntrees: 200 nodesize: 1, ntrees: 300 nodesize: 1, ntrees: 100 nodesize: 5, ntrees: 200 nodesize: 5, ntrees: 300 nodesize: 5, ntrees: 100 nodesize: 10, ntrees: 200 nodesize: 10, ntrees: 300 nodesize: 10 \n",
    "# Number of resamples: 5 \n",
    "# \n",
    "# ROC \n",
    "# Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
    "# ntrees: 100 nodesize: 1  0.9999405 0.9999617 0.9999657 0.9999663 0.9999702 0.9999933    0\n",
    "# ntrees: 200 nodesize: 1  0.9999699 0.9999753 0.9999773 0.9999803 0.9999853 0.9999939    0\n",
    "# ntrees: 300 nodesize: 1  0.9999731 0.9999765 0.9999802 0.9999824 0.9999886 0.9999936    0\n",
    "# ntrees: 100 nodesize: 5  0.9999227 0.9999609 0.9999638 0.9999619 0.9999703 0.9999919    0\n",
    "# ntrees: 200 nodesize: 5  0.9999561 0.9999708 0.9999848 0.9999781 0.9999886 0.9999902    0\n",
    "# ntrees: 300 nodesize: 5  0.9999670 0.9999765 0.9999881 0.9999827 0.9999903 0.9999913    0\n",
    "# ntrees: 100 nodesize: 10 0.9998941 0.9999307 0.9999485 0.9999434 0.9999679 0.9999759    0\n",
    "# ntrees: 200 nodesize: 10 0.9999529 0.9999577 0.9999689 0.9999712 0.9999879 0.9999886    0\n",
    "# ntrees: 300 nodesize: 10 0.9999686 0.9999717 0.9999800 0.9999809 0.9999917 0.9999926    0\n",
    "# \n",
    "# Sens \n",
    "#                          Min. 1st Qu. Median Mean 3rd Qu. Max. NA's\n",
    "# ntrees: 100 nodesize: 1     1       1      1    1       1    1    0\n",
    "# ntrees: 200 nodesize: 1     1       1      1    1       1    1    0\n",
    "# ntrees: 300 nodesize: 1     1       1      1    1       1    1    0\n",
    "# ntrees: 100 nodesize: 5     1       1      1    1       1    1    0\n",
    "# ntrees: 200 nodesize: 5     1       1      1    1       1    1    0\n",
    "# ntrees: 300 nodesize: 5     1       1      1    1       1    1    0\n",
    "# ntrees: 100 nodesize: 10    1       1      1    1       1    1    0\n",
    "# ntrees: 200 nodesize: 10    1       1      1    1       1    1    0\n",
    "# ntrees: 300 nodesize: 10    1       1      1    1       1    1    0\n",
    "# \n",
    "# Spec \n",
    "# Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's\n",
    "# ntrees: 100 nodesize: 1  0.9983303 0.9985201 0.9988237 0.9986719 0.9988237 0.9988616    0\n",
    "# ntrees: 200 nodesize: 1  0.9982923 0.9985201 0.9988236 0.9986567 0.9988237 0.9988237    0\n",
    "# ntrees: 300 nodesize: 1  0.9983303 0.9985201 0.9988236 0.9986643 0.9988237 0.9988237    0\n",
    "# ntrees: 100 nodesize: 5  0.9981785 0.9982545 0.9986719 0.9985049 0.9986719 0.9987477    0\n",
    "# ntrees: 200 nodesize: 5  0.9981406 0.9983304 0.9986719 0.9985277 0.9987477 0.9987478    0\n",
    "# ntrees: 300 nodesize: 5  0.9981785 0.9983304 0.9986339 0.9985125 0.9987098 0.9987098    0\n",
    "# ntrees: 100 nodesize: 10 0.9981027 0.9981406 0.9985580 0.9983986 0.9985580 0.9986339    0\n",
    "# ntrees: 200 nodesize: 10 0.9980647 0.9981027 0.9985200 0.9983986 0.9986339 0.9986719    0\n",
    "# ntrees: 300 nodesize: 10 0.9980647 0.9981406 0.9985959 0.9984214 0.9986339 0.9986719    0\n",
    "\n",
    "# to get the best average performance for each model\n",
    "lapply(data_maxnode, function(x) x$results[x$results$ROC == max(x$results$ROC),])\n",
    "# $`ntrees: 100 nodesize: 1`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999663    1 0.9986719 1.893113e-05      0 0.0002354651\n",
    "# \n",
    "# $`ntrees: 200 nodesize: 1`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999803    1 0.9986567 9.381088e-06      0 0.0002423985\n",
    "# \n",
    "# $`ntrees: 300 nodesize: 1`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD      SpecSD\n",
    "# 1    3 0.9999824    1 0.9986643 8.524244e-06      0 0.000228325\n",
    "# \n",
    "# $`ntrees: 100 nodesize: 5`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999619    1 0.9985049 2.504672e-05      0 0.0002664466\n",
    "# \n",
    "# $`ntrees: 200 nodesize: 5`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999781    1 0.9985277 1.449106e-05      0 0.0002767895\n",
    "# \n",
    "# $`ntrees: 300 nodesize: 5`\n",
    "# mtry       ROC Sens      Spec       ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999827    1 0.9985125 1.05527e-05      0 0.0002435808\n",
    "# \n",
    "# $`ntrees: 100 nodesize: 10`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999434    1 0.9983986 3.270358e-05      0 0.0002551262\n",
    "# \n",
    "# $`ntrees: 200 nodesize: 10`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999712    1 0.9983986 1.660777e-05      0 0.0002932105\n",
    "# \n",
    "# $`ntrees: 300 nodesize: 10`\n",
    "# mtry       ROC Sens      Spec        ROCSD SensSD       SpecSD\n",
    "# 1    3 0.9999809    1 0.9984214 1.106868e-05      0 0.0002934552\n",
    "\n",
    "# View the best tuning parameters\n",
    "# print(rf_tune$bestTune)  # Best combination of parameters\n",
    "\n",
    "# 5. Model summary and performance evaluation\n",
    "# print(rf_tune)  # Output results of cross-validation\n",
    "\n",
    "# RF model with tuned parameters\n",
    "rf_model <- randomForest(as.factor(class) ~., data=train_smote, ntree= 300, \n",
    "                      mtry=3, nodesize =5, importance=TRUE)\n",
    "# Variable Importance\n",
    "# varImpPlot(rf_tune$finalModel, main = \"Variable Importance (Tuned Random Forest)\")\n",
    "varImpPlot(rf_model, main = \"Variable Importance (Tuned Random Forest)\")\n",
    "\n",
    "# 6. Prediction on Test Data\n",
    "rf.pred <- predict(rf_model, Data1_test_10_processed, type = \"prob\")[, 2] # Get probabilities for the positive class\n",
    "rf.pred_class <- predict(rf_model, Data1_test_10_processed, type = \"class\")\n",
    "\n",
    "# Confusion Matrix and Performance Evaluation for Random Forest\n",
    "conf_matrix_rf <- confusionMatrix(as.factor(rf.pred_class), as.factor(Data1_test_10_processed$DUMMY_KILLED))\n",
    "print(conf_matrix_rf)\n",
    "# Confusion Matrix and Statistics\n",
    "# \n",
    "# Reference\n",
    "# Prediction     0     1\n",
    "# 0 32966    43\n",
    "# 1     0     0\n",
    "# \n",
    "# Accuracy : 0.9987          \n",
    "# 95% CI : (0.9982, 0.9991)\n",
    "# No Information Rate : 0.9987          \n",
    "# P-Value [Acc > NIR] : 0.5404          \n",
    "# \n",
    "# Kappa : 0               \n",
    "# \n",
    "# Mcnemar's Test P-Value : 1.504e-10       \n",
    "#                                           \n",
    "#             Sensitivity : 1.0000          \n",
    "#             Specificity : 0.0000          \n",
    "#          Pos Pred Value : 0.9987          \n",
    "#          Neg Pred Value :    NaN          \n",
    "#              Prevalence : 0.9987          \n",
    "#          Detection Rate : 0.9987          \n",
    "#    Detection Prevalence : 1.0000          \n",
    "#       Balanced Accuracy : 0.5000          \n",
    "#                                           \n",
    "#        'Positive' Class : 0        \n",
    "\n",
    "# ROC Curve and AUC for Random Forest\n",
    "prediction_rf <- prediction(rf.pred, Data1_test_10_processed$DUMMY_KILLED)\n",
    "perf_rf <- performance(prediction_rf, \"tpr\", \"fpr\")\n",
    "plot(perf_rf, main = \"ROC Curve (Tuned Random Forest)\")\n",
    "abline(a = 0, b = 1, lty = 2) # Diagonal line for reference\n",
    "auc_rf <- performance(prediction_rf, \"auc\")@y.values[[1]]\n",
    "cat(\"AUC on Test Data (Tuned Random Forest):\", round(auc_rf, 4), \"\\n\")\n",
    "# AUC on Test Data (Tuned Random Forest): 0.5662\n",
    "\n",
    "##--------------------F1 score as metrics-----------------------\n",
    "# Define custom trainControl using F1 score\n",
    "ctrl_F1 <- trainControl(method = \"cv\",\n",
    "                     number = 3,\n",
    "                     search = 'grid',\n",
    "                     classProbs = TRUE,\n",
    "                     savePredictions = \"final\",\n",
    "                     index = cv_folds,\n",
    "                     summaryFunction = function(data, lev = NULL, model = NULL) {\n",
    "                       predicted <- data$pred\n",
    "                       actual <- data$obs\n",
    "                       f1 <- F1_Score(predicted, actual)\n",
    "                       return(c(F1 = f1))\n",
    "                     },\n",
    "                     verboseIter = TRUE)\n",
    "\n",
    "# Random Forest Parameter Tuning\n",
    "data_maxnode_F1 <- vector(\"list\", nrow(param_grid))\n",
    "for(i in 1:nrow(param_grid)){\n",
    "  ntree <- param_grid[i,1]\n",
    "  nodesize <- param_grid[i,2]\n",
    "  set.seed(333)\n",
    "  \n",
    "  # Train Random Forest model with F1 score as the metric\n",
    "  rf_model_F1 <- train(make.names(class) ~ ., \n",
    "                    data = train_smote,\n",
    "                    method = \"rf\",\n",
    "                    importance = TRUE,\n",
    "                    metric = \"F1\",  # Specify F1 as the evaluation metric\n",
    "                    trControl = ctrl_F1,\n",
    "                    tuneGrid = tuneGrid,\n",
    "                    ntree = ntree,\n",
    "                    nodesize = nodesize)\n",
    "  \n",
    "  # Store the model\n",
    "  data_maxnode_F1[[i]] <- rf_model_F1\n",
    "}\n",
    "\n",
    "# Assign model names based on parameters\n",
    "names(data_maxnode_F1) <- paste(\"ntrees:\", param_grid$ntrees,\n",
    "                             \"nodesize:\", param_grid$nodesize)\n",
    "\n",
    "# 5. Evaluate results\n",
    "results_mtry_F1 <- resamples(data_maxnode_F1)\n",
    "summary(results_mtry_F1)\n",
    "# \n",
    "# + Fold1: mtry=3 \n",
    "# - Fold1: mtry=3 \n",
    "# + Fold1: mtry=5 \n",
    "# - Fold1: mtry=5 \n",
    "# + Fold2: mtry=3 \n",
    "# - Fold2: mtry=3 \n",
    "# + Fold2: mtry=5 \n",
    "# - Fold2: mtry=5 \n",
    "# + Fold3: mtry=3 \n",
    "# - Fold3: mtry=3 \n",
    "# + Fold3: mtry=5 \n",
    "# - Fold3: mtry=5 \n",
    "# + Fold4: mtry=3 \n",
    "# - Fold4: mtry=3 \n",
    "# + Fold4: mtry=5 \n",
    "# - Fold4: mtry=5 \n",
    "# + Fold5: mtry=3 \n",
    "# - Fold5: mtry=3 \n",
    "# + Fold5: mtry=5 \n",
    "# - Fold5: mtry=5 \n",
    "# Aggregating results\n",
    "# Something is wrong; all the F1 metric values are missing:\n",
    "#   F1     \n",
    "# Min.   : NA  \n",
    "# 1st Qu.: NA  \n",
    "# Median : NA  \n",
    "# Mean   :NaN  \n",
    "# 3rd Qu.: NA  \n",
    "# Max.   : NA  \n",
    "# NA's   :2    \n",
    "# Error: Stopping\n",
    "# In addition: Warning message:\n",
    "# In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :\n",
    "#   There were missing values in resampled performance measures.\n",
    "\n",
    "\n",
    "#####################Boosting (GBM Model)###########################################\n",
    "# --------------------------- Boosting (GBM Model) ---------------------------------\n",
    "# Define parameter grid for tuning\n",
    "gbmGrid <- expand.grid(\n",
    "  n.trees = c(50, 100, 500), \n",
    "  interaction.depth = c(1, 3, 5), \n",
    "  shrinkage = c(0.01, 0.1), \n",
    "  n.minobsinnode = c(1, 5, 10)\n",
    ")\n",
    "\n",
    "fitControl <- trainControl(\n",
    "  method = \"cv\",\n",
    "  number = 3,\n",
    "  classProbs = TRUE,\n",
    "  sampling = \"smote\", # Apply SMOTE resampling\n",
    "  summaryFunction = twoClassSummary # Important for AUC\n",
    ")\n",
    "\n",
    "set.seed(222)\n",
    "# Fit Boosting model using caret\n",
    "gbmFit <- train(\n",
    "  make.names(class) ~ ., \n",
    "  data = train_smote, \n",
    "  method = \"gbm\", \n",
    "  trControl = fitControl, \n",
    "  tuneGrid = gbmGrid, \n",
    "  metric = \"ROC\", \n",
    "  verbose = FALSE\n",
    ")\n",
    "\n",
    "\n",
    "# Print Boosting results\n",
    "print(gbmFit)\n",
    "plot(gbmFit)\n",
    "\n",
    "# Stochastic Gradient Boosting \n",
    "# \n",
    "# 263631 samples\n",
    "# 20 predictor\n",
    "# 2 classes: 'X0', 'X1' \n",
    "# \n",
    "# No pre-processing\n",
    "# Resampling: Cross-Validated (3 fold) \n",
    "# Summary of sample sizes: 175754, 175754, 175754 \n",
    "# Addtional sampling using SMOTE\n",
    "# \n",
    "# Resampling results across tuning parameters:\n",
    "#   \n",
    "#   shrinkage  interaction.depth  n.minobsinnode  n.trees  ROC        Sens       Spec     \n",
    "# 0.01       1                   1               50      0.7419044  0.5348000  0.9449238\n",
    "# 0.01       1                   1              100      0.8440745  0.5348000  0.9449390\n",
    "# 0.01       1                   1              500      0.9312761  0.7731974  0.9045559\n",
    "# 0.01       1                   5               50      0.7419044  0.5348000  0.9449390\n",
    "# 0.01       1                   5              100      0.8519786  0.5348000  0.9449390\n",
    "# 0.01       1                   5              500      0.9304077  0.7690190  0.9112118\n",
    "# 0.01       1                  10               50      0.7419044  0.5348000  0.9449238\n",
    "# 0.01       1                  10              100      0.8440715  0.5348000  0.9449238\n",
    "# 0.01       1                  10              500      0.9311933  0.7671156  0.9105060\n",
    "# 0.01       3                   1               50      0.8783150  0.7625883  0.8270455\n",
    "# 0.01       3                   1              100      0.9363616  0.7633391  0.8828047\n",
    "# 0.01       3                   1              500      0.9916992  0.9671945  0.9504869\n",
    "# 0.01       3                   5               50      0.8783164  0.7625883  0.8270379\n",
    "# 0.01       3                   5              100      0.9360081  0.7638775  0.8827971\n",
    "# 0.01       3                   5              500      0.9916080  0.9669139  0.9519971\n",
    "# 0.01       3                  10               50      0.8783150  0.7625883  0.8270379\n",
    "# 0.01       3                  10              100      0.9353050  0.7644690  0.8827896\n",
    "# 0.01       3                  10              500      0.9917287  0.9666636  0.9516860\n",
    "# 0.01       5                   1               50      0.9161816  0.9212015  0.7879223\n",
    "# 0.01       5                   1              100      0.9511398  0.9094397  0.8778413\n",
    "# 0.01       5                   1              500      0.9979008  0.9881472  0.9717675\n",
    "# 0.01       5                   5               50      0.9188938  0.9212015  0.7879223\n",
    "# 0.01       5                   5              100      0.9520559  0.9094018  0.8778337\n",
    "# 0.01       5                   5              500      0.9979252  0.9884051  0.9719117\n",
    "# 0.01       5                  10               50      0.9182053  0.9212015  0.7879071\n",
    "# 0.01       5                  10              100      0.9511471  0.9100009  0.8778413\n",
    "# 0.01       5                  10              500      0.9979405  0.9884961  0.9738166\n",
    "# 0.10       1                   1               50      0.9288928  0.7781797  0.9140958\n",
    "# 0.10       1                   1              100      0.9593495  0.8480753  0.9102100\n",
    "# 0.10       1                   1              500      0.9913700  0.9702354  0.9546838\n",
    "# 0.10       1                   5               50      0.9299191  0.7782631  0.9097167\n",
    "# 0.10       1                   5              100      0.9594490  0.8508736  0.9089274\n",
    "# 0.10       1                   5              500      0.9913177  0.9704022  0.9549722\n",
    "# 0.10       1                  10               50      0.9290828  0.7803789  0.9107792\n",
    "# 0.10       1                  10              100      0.9597144  0.8493190  0.9110828\n",
    "# 0.10       1                  10              500      0.9914226  0.9707283  0.9542436\n",
    "# 0.10       3                   1               50      0.9915706  0.9673461  0.9464417\n",
    "# 0.10       3                   1              100      0.9979329  0.9878742  0.9691719\n",
    "# 0.10       3                   1              500      0.9997049  0.9999924  0.9986719\n",
    "# 0.10       3                   5               50      0.9916378  0.9677177  0.9492574\n",
    "# 0.10       3                   5              100      0.9979328  0.9883975  0.9680107\n",
    "# 0.10       3                   5              500      0.9997072  1.0000000  0.9986794\n",
    "# 0.10       3                  10               50      0.9913697  0.9657915  0.9478761\n",
    "# 0.10       3                  10              100      0.9979569  0.9878970  0.9694527\n",
    "# 0.10       3                  10              500      0.9997026  1.0000000  0.9986870\n",
    "# 0.10       5                   1               50      0.9976952  0.9882231  0.9716764\n",
    "# 0.10       5                   1              100      0.9996117  0.9984530  0.9915454\n",
    "# 0.10       5                   1              500      0.9995530  0.9996587  0.9987022\n",
    "# 0.10       5                   5               50      0.9980666  0.9889814  0.9734220\n",
    "# 0.10       5                   5              100      0.9996068  0.9985895  0.9912494\n",
    "# 0.10       5                   5              500      0.9997337  0.9998332  0.9986719\n",
    "# 0.10       5                  10               50      0.9978561  0.9887008  0.9712059\n",
    "# 0.10       5                  10              100      0.9996029  0.9983923  0.9913405\n",
    "# 0.10       5                  10              500      0.9997468  0.9999242  0.9986946\n",
    "# \n",
    "# ROC was used to select the optimal model using the largest value.\n",
    "# The final values used for the model were n.trees = 500, interaction.depth = 5, shrinkage =\n",
    "#   0.1 and n.minobsinnode = 10.\n",
    "\n",
    "# optimal tune values\n",
    "gbmFit$finalModel$tuneValue\n",
    "# n.trees interaction.depth shrinkage n.minobsinnode\n",
    "# 54     500                 5       0.1             10\n",
    "\n",
    "whichTwoPct <- tolerance(gbmFit$results, metric = \"ROC\", \n",
    "                         tol = 2, maximize = TRUE)  \n",
    "cat(\"best model within 2 pct of best:\")\n",
    "gbmFit$results[whichTwoPct,1:6]\n",
    "# shrinkage interaction.depth n.minobsinnode n.trees       ROC      Sens\n",
    "# 37       0.1                 3              1      50 0.9915706 0.9673461\n",
    "\n",
    "# optimal model\n",
    "gbmFit_optimal <- gbm(\n",
    "  class ~ ., \n",
    "  data = train_smote, \n",
    "  distribution = 'bernoulli',\n",
    "  n.trees = 50,  # Optimal n.trees\n",
    "  shrinkage = 0.1,  # Optimal shrinkage\n",
    "  interaction.depth = 3,  # Optimal interaction depth\n",
    "  n.minobsinnode = 1,  # Optimal minobsinnode\n",
    "  cv.folds = 10  # Cross-validation to assess the model's generalization\n",
    ")\n",
    "\n",
    "summary(gbmFit_optimal)\n",
    "\n",
    "## Training error\n",
    "predgbm <- predict(gbm_model,\n",
    "                   newdata = train_smote, \n",
    "                   n.trees=50, \n",
    "                   shrinkage = 0.1,\n",
    "                   interaction.depth = 3,\n",
    "                   n.minobsinnode = 1,\n",
    "                   type=\"response\")\n",
    "y1hat <- ifelse(predgbm < 0.5, 0, 1)\n",
    "sum(y1hat != y1)/length(y1)  ##Training error = 0.09162896\n",
    "\n",
    "## Testing Error\n",
    "y2hat <- ifelse(predict(gbm_model,newdata = Data1_test_10_processed[,-1], n.trees=50, type=\"response\") < 0.5, 0, 1)\n",
    "mean(y2hat != y2) \n",
    "## Testing error = 0.1357466\n",
    "\n",
    "# Now compute the confusion matrix\n",
    "conf_matrix_gbm <- confusionMatrix(gbm_pred_class, factor(Data1_test_10_processed$DUMMY_KILLED, levels = c(0, 1)))\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(conf_matrix_gbm)\n",
    "\n",
    "\n",
    "# ROC Curve and AUC for Boosting\n",
    "prediction_gbm <- prediction(gbm_pred, Data1_test_processed$DUMMY_KILLED)\n",
    "perf_gbm <- performance(prediction_gbm, \"tpr\", \"fpr\")\n",
    "plot(perf_gbm, main = \"ROC Curve (Boosting)\")\n",
    "abline(a = 0, b = 1, lty = 2) # Diagonal line for reference\n",
    "auc_gbm <- performance(prediction_gbm, \"auc\")@y.values[[1]]\n",
    "cat(\"AUC on Test Data (Boosting):\", round(auc_gbm, 4), \"\\n\")\n",
    "# AUC on Test Data (Boosting): 0.6813 \n",
    "\n",
    "# 3. **Variable Importance for Boosting**\n",
    "# Variable importance from the trained GBM model\n",
    "importance_gbm <- varImp(gbmFit, scale = FALSE)\n",
    "print(importance_gbm)\n",
    "\n",
    "# Plot variable importance for Boosting\n",
    "plot(importance_gbm, main = \"Variable Importance (Boosting)\")\n",
    "\n",
    "# Optional: You can adjust the importance plot parameters (like top variables) if needed\n",
    "\n",
    "\n",
    "#####################SVM###########################################\n",
    "# --------------------------- SVM ---------------------------------\n",
    "# Tune SVM hyperparameters (cost and gamma)\n",
    "tune_out <- tune.svm(\n",
    "  x = Data1_train_processed[, -which(names(Data1_train_processed) == \"DUMMY_KILLED\")],\n",
    "  y = Data1_train_processed$DUMMY_KILLED,\n",
    "  gamma = 10^(-3:3),\n",
    "  cost = c(0.01, 0.1, 1, 10, 100, 1000),\n",
    "  kernel = \"radial\"\n",
    ")\n",
    "\n",
    "# Fit SVM model with best parameters\n",
    "svm_model <- svm(\n",
    "  as.factor(class) ~ ., \n",
    "  data = train_smote, \n",
    "  method = \"C-classification\", \n",
    "  kernel = \"radial\", \n",
    "  cost = tune_out$best.parameters$cost, \n",
    "  gamma = tune_out$best.parameters$gamma\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "svm_pred <- predict(svm_model, Data1_test_10_processed, type = \"response\")\n",
    "svm_pred_class <- predict(svm_model, Data1_test_10_processed, type = \"class\")\n",
    "\n",
    "# Confusion Matrix and Performance Evaluation for SVM\n",
    "conf_matrix_svm <- confusionMatrix(svm_pred_class, Data1_test_10_processed$DUMMY_KILLED)\n",
    "print(conf_matrix_svm)\n",
    "\n",
    "# ROC Curve and AUC for SVM\n",
    "prediction_svm <- prediction(as.numeric(svm_pred_class), Data1_test_10_processed$DUMMY_KILLED)\n",
    "perf_svm <- performance(prediction_svm, \"tpr\", \"fpr\")\n",
    "plot(perf_svm, main = \"ROC Curve (SVM)\")\n",
    "abline(a = 0, b = 1, lty = 2) # Diagonal line for reference\n",
    "auc_svm <- performance(prediction_svm, \"auc\")@y.values[[1]]\n",
    "cat(\"AUC on Test Data (SVM):\", round(auc_svm, 4), \"\\n\")\n",
    "\n",
    "\n",
    "# Feature Importance for SVM \n",
    "M <- fit(DUMMY_KILLED ~ ., data = Data1_train_10_processed, model = \"svm\", kpar = list(sigma = tune_out$best.parameters$gamma), C = tune_out$best.parameters$cost)\n",
    "svm_imp <- Importance(M, data = Data1_train_10_processed)\n",
    "print(svm_imp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
